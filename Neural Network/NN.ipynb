{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "\n",
    "    def __init__(self, n_input, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "\n",
    "        # Intialize weights as gaussian distribution\n",
    "        self.weights = 0.01 * np.random.randn(n_input, n_neurons)\n",
    "        self.weight_momentums = self.weights\n",
    "        # Initialize biases as zero\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        \n",
    "        # Gradients on Parameters\n",
    "        self.dweights = np.dot(self.inputs.T , dvalues)\n",
    "        self.dbiases = np.sum(dvalues , axis = 0 , keepdims=True )\n",
    "\n",
    "        # L1 Regularization \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            # Initializing dL1 with same number of elements as the weights array\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "\n",
    "            # Setting any gradients less than 0 to -1\n",
    "            dL1[self.weights < 0] = -1\n",
    "\n",
    "            # Finding Derivatives of the weights\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        \n",
    "        # L2 Regularization\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            # Similar but 2 cause of (w^2) instead of |w|\n",
    "            self.dweights += 2* self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        \n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2* self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "    \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "\n",
    "    # Dropout Layer to select any percent of neurons for each iteration to avoid one neuron taking all the weight.\n",
    "    def __init__(self, rate):\n",
    "\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Generating a mask to choose a certain amount of neurons while keeping the output of the activation function the same\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.binary_mask = np.random.binomial(1, self.rate , size=inputs.shape) / self.rate\n",
    "\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Relu:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # ReLU Function -> max(0, x)\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        # Making a copy for modification\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient for any negative values\n",
    "        self.dinputs[self.inputs <= 0 ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Softmax Function\n",
    "        ex = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilties = ex / np.sum(ex, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculates negative log likelihoods\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        regularization_loss = 0\n",
    "\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights**2)\n",
    "\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases**2)\n",
    "        \n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical_Cross_Entropy_loss(Loss):\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # Number of Samples\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clipping so that no value is 1 or 0.\n",
    "        y_pred_clipped = np.clip(y_pred, 0.0000001 , 1 - 0.0000001)\n",
    "\n",
    "        # If y_true is a 1D vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples) , y_true]\n",
    "        \n",
    "        # If y_true is one hot encoded\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "\n",
    "            # As y_true is one hot encoded,\n",
    "            # Multiplying by it would mean , only the relevant prediction is taken\n",
    "            # Then we sum it along the column.\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true , axis=1)\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Length of Samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # Length of labels of a sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            # One hot encoding \n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "    \n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossEntropy:\n",
    "\n",
    "    # Combining Softmax Activation and CrossEntropy Categorical Loss\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Categorical_Cross_Entropy_loss()\n",
    "    \n",
    "    def forward(self, inputs , y_true):\n",
    "\n",
    "        # Activation Function\n",
    "        self.activation.forward(inputs)\n",
    "\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        # Calculate and return loss\n",
    "        return self.loss.calculate(self.output , y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If one hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true , axis = 1)\n",
    "\n",
    "        # Copy for modification\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate Gradient\n",
    "        self.dinputs[range(samples) , y_true] -= 1\n",
    "        # Normalize gradien\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "Loss:  1.0986104\n",
      "acc:  0.34\n",
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Training the Neural Network without any optimizers\n",
    "\n",
    "# Spiral Dataset\n",
    "X,y = spiral_data(samples =100 , classes=3)\n",
    "\n",
    "# First Dense Layer\n",
    "dense1 = Dense_Layer(2,3)\n",
    "\n",
    "# First Activation Layer\n",
    "activation1= Activation_Relu()\n",
    "\n",
    "# Second Dense Layer\n",
    "dense2 = Dense_Layer(3,3)\n",
    "\n",
    "# Loss/Softmax Activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Forward pass data through first layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Pass the output of first layer to activation function\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Pass the output of first activation to second layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Pass the output of second layer to softmax activation and calculate losses\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "print(loss_activation.output[:5])\n",
    "print('Loss: ' , loss)\n",
    "\n",
    "# Calculate accuracy from output of softmax activation and targets\n",
    "# Calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output , axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "print('acc: ', accuracy)\n",
    "\n",
    "loss_activation.backward(loss_activation.output , y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    "\n",
    "    # Gradient Descent Optimizer\n",
    "    def __init__(self, learning_rate=1):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD_Decay:\n",
    "\n",
    "    # Gradient descent optimizer with decay\n",
    "    # Essentially decreasing the regularization parameter(learning rate) as the iterations increase\n",
    "    def __init__(self, learning_rate=1, decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations= 0\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    # Stochastic Gradient Descent Optimizer\n",
    "    # Using Momentum to keep track of past gradients to reduce redundant step and converge faster\n",
    "    def __init__(self , learning_rate=1. , decay=0. , momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                  (1. / (1. + self.decay * self.iterations)  )\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # Creating a momentum array if there is none\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weight)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # Updating the weight updates  and bias updates based on momentum and previous gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "             self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "             self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        # Updating weights and biases\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    \n",
    "    def post_update_params(self):\n",
    "            self.iterations += 1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSPROP:\n",
    "\n",
    "    # RMSPROP Optimizer\n",
    "    # Introducing concept of cache\n",
    "    # Similar to Momentum but used in updating weights\n",
    "    # Higher weight values get updated less i.e less learning rate for them\n",
    "    # Lower weight values get updated more i.e more learning rate for them\n",
    "    # This makes sure all neurons learn and\n",
    "    # A High Discrepancy does not occur in their values.\n",
    "    \n",
    "    def __init__(self , learning_rate=1. , decay=0. , epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                  (1. / (1. + self.decay * self.iterations)  )\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "    \n",
    "            if not hasattr(layer, 'weight_cache'):\n",
    "                layer.weight_cache=np.zeros_like(layer.weights)\n",
    "                layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "            # If you add a factor to prioritise the present cache more than the gradient squared,\n",
    "            # Its RMSProp Optimizer\n",
    "            # (rho * cache**2 + (1-rho) * gradd**2)\n",
    "            layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1-self.rho) * layer.dweights**2 \n",
    "            \n",
    "            layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1-self.rho) * layer.dbiases**2 \n",
    "\n",
    "            layer.weights += -self.current_learning_rate * \\\n",
    "                layer.dweights / \\\n",
    "                (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "            \n",
    "            layer.biases += -self.current_learning_rate * \\\n",
    "                layer.dbiases / \\\n",
    "                (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            \n",
    "    def post_update_params(self):\n",
    "            self.iterations += 1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "\n",
    "    # Adam Optimizer\n",
    "    # Combines concept of Momentum and Cache\n",
    "    # Most widely used Optimizer generally\n",
    "    def __init__(self , learning_rate=0.001 , decay=0. , epsilon=1e-7, rho=0.999, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                  (1. / (1. + self.decay * self.iterations)  )\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "    \n",
    "            if not hasattr(layer, 'weight_cache'):\n",
    "                layer.weight_cache=np.zeros_like(layer.weights)\n",
    "                layer.bias_cache = np.zeros_like(layer.biases)\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            layer.weight_momentums = self.momentum * layer.weight_momentums + (1-self.momentum) * layer.dweights\n",
    "            layer.bias_momentums = self.momentum * layer.bias_momentums + (1-self.momentum) * layer.dbiases\n",
    "\n",
    "            weight_momentums_corrected = layer.weight_momentums / (1 - self.momentum ** (self.iterations + 1))\n",
    "            bias_momentums_corrected = layer.bias_momentums / (1 - self.momentum ** (self.iterations + 1))\n",
    "            \n",
    "            layer.weight_cache = self.rho * layer.weight_cache + (1-self.rho) * layer.dweights**2 \n",
    "            layer.bias_cache = self.rho * layer.bias_cache + (1-self.rho) * layer.dbiases**2 \n",
    "\n",
    "            weight_cache_corrected = layer.weight_cache / (1- self.rho**(self.iterations + 1))\n",
    "            bias_cache_corrected = layer.bias_cache / (1- self.rho**(self.iterations + 1))\n",
    "\n",
    "\n",
    "            layer.weights += -self.current_learning_rate * \\\n",
    "                weight_momentums_corrected / \\\n",
    "                (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "            \n",
    "            layer.biases += -self.current_learning_rate * \\\n",
    "                bias_momentums_corrected / \\\n",
    "                (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "            \n",
    "    def post_update_params(self):\n",
    "            self.iterations += 1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss:1.099 (data_loss:1.099 , regularization_loss:0.000), lr: 0.02\n",
      "epoch: 100, acc: 0.669, loss:0.884 (data_loss:0.852 , regularization_loss:0.032), lr: 0.019999010049002574\n",
      "epoch: 200, acc: 0.771, loss:0.695 (data_loss:0.616 , regularization_loss:0.079), lr: 0.019998010197985302\n",
      "epoch: 300, acc: 0.809, loss:0.616 (data_loss:0.519 , regularization_loss:0.097), lr: 0.019997010446938183\n",
      "epoch: 400, acc: 0.829, loss:0.570 (data_loss:0.467 , regularization_loss:0.103), lr: 0.01999601079584623\n",
      "epoch: 500, acc: 0.842, loss:0.538 (data_loss:0.433 , regularization_loss:0.104), lr: 0.01999501124469445\n",
      "epoch: 600, acc: 0.855, loss:0.514 (data_loss:0.411 , regularization_loss:0.104), lr: 0.01999401179346786\n",
      "epoch: 700, acc: 0.859, loss:0.496 (data_loss:0.394 , regularization_loss:0.102), lr: 0.01999301244215147\n",
      "epoch: 800, acc: 0.853, loss:0.484 (data_loss:0.384 , regularization_loss:0.100), lr: 0.0199920131907303\n",
      "epoch: 900, acc: 0.871, loss:0.467 (data_loss:0.368 , regularization_loss:0.099), lr: 0.019991014039189386\n",
      "epoch: 1000, acc: 0.876, loss:0.453 (data_loss:0.355 , regularization_loss:0.098), lr: 0.019990014987513734\n",
      "epoch: 1100, acc: 0.873, loss:0.442 (data_loss:0.346 , regularization_loss:0.096), lr: 0.01998901603568839\n",
      "epoch: 1200, acc: 0.870, loss:0.434 (data_loss:0.340 , regularization_loss:0.094), lr: 0.019988017183698373\n",
      "epoch: 1300, acc: 0.869, loss:0.427 (data_loss:0.335 , regularization_loss:0.092), lr: 0.01998701843152872\n",
      "epoch: 1400, acc: 0.881, loss:0.421 (data_loss:0.331 , regularization_loss:0.090), lr: 0.019986019779164473\n",
      "epoch: 1500, acc: 0.872, loss:0.413 (data_loss:0.324 , regularization_loss:0.089), lr: 0.019985021226590672\n",
      "epoch: 1600, acc: 0.875, loss:0.407 (data_loss:0.320 , regularization_loss:0.087), lr: 0.01998402277379235\n",
      "epoch: 1700, acc: 0.885, loss:0.402 (data_loss:0.317 , regularization_loss:0.086), lr: 0.01998302442075457\n",
      "epoch: 1800, acc: 0.875, loss:0.397 (data_loss:0.312 , regularization_loss:0.084), lr: 0.019982026167462367\n",
      "epoch: 1900, acc: 0.880, loss:0.391 (data_loss:0.308 , regularization_loss:0.083), lr: 0.019981028013900805\n",
      "epoch: 2000, acc: 0.877, loss:0.390 (data_loss:0.308 , regularization_loss:0.082), lr: 0.019980029960054924\n",
      "epoch: 2100, acc: 0.877, loss:0.383 (data_loss:0.302 , regularization_loss:0.081), lr: 0.019979032005909798\n",
      "epoch: 2200, acc: 0.883, loss:0.380 (data_loss:0.300 , regularization_loss:0.080), lr: 0.01997803415145048\n",
      "epoch: 2300, acc: 0.879, loss:0.377 (data_loss:0.298 , regularization_loss:0.079), lr: 0.019977036396662037\n",
      "epoch: 2400, acc: 0.884, loss:0.372 (data_loss:0.294 , regularization_loss:0.078), lr: 0.019976038741529537\n",
      "epoch: 2500, acc: 0.881, loss:0.370 (data_loss:0.293 , regularization_loss:0.077), lr: 0.01997504118603805\n",
      "epoch: 2600, acc: 0.891, loss:0.367 (data_loss:0.291 , regularization_loss:0.076), lr: 0.01997404373017264\n",
      "epoch: 2700, acc: 0.883, loss:0.363 (data_loss:0.288 , regularization_loss:0.075), lr: 0.0199730463739184\n",
      "epoch: 2800, acc: 0.889, loss:0.361 (data_loss:0.287 , regularization_loss:0.074), lr: 0.019972049117260395\n",
      "epoch: 2900, acc: 0.889, loss:0.360 (data_loss:0.287 , regularization_loss:0.073), lr: 0.019971051960183714\n",
      "epoch: 3000, acc: 0.887, loss:0.356 (data_loss:0.283 , regularization_loss:0.072), lr: 0.019970054902673444\n",
      "epoch: 3100, acc: 0.886, loss:0.353 (data_loss:0.281 , regularization_loss:0.072), lr: 0.019969057944714663\n",
      "epoch: 3200, acc: 0.885, loss:0.352 (data_loss:0.281 , regularization_loss:0.071), lr: 0.019968061086292475\n",
      "epoch: 3300, acc: 0.889, loss:0.349 (data_loss:0.279 , regularization_loss:0.070), lr: 0.019967064327391967\n",
      "epoch: 3400, acc: 0.885, loss:0.348 (data_loss:0.279 , regularization_loss:0.069), lr: 0.019966067667998237\n",
      "epoch: 3500, acc: 0.884, loss:0.346 (data_loss:0.277 , regularization_loss:0.069), lr: 0.019965071108096383\n",
      "epoch: 3600, acc: 0.886, loss:0.345 (data_loss:0.276 , regularization_loss:0.068), lr: 0.01996407464767152\n",
      "epoch: 3700, acc: 0.883, loss:0.343 (data_loss:0.275 , regularization_loss:0.068), lr: 0.019963078286708732\n",
      "epoch: 3800, acc: 0.891, loss:0.340 (data_loss:0.273 , regularization_loss:0.067), lr: 0.019962082025193145\n",
      "epoch: 3900, acc: 0.885, loss:0.338 (data_loss:0.272 , regularization_loss:0.066), lr: 0.019961085863109868\n",
      "epoch: 4000, acc: 0.891, loss:0.337 (data_loss:0.271 , regularization_loss:0.066), lr: 0.019960089800444013\n",
      "epoch: 4100, acc: 0.886, loss:0.340 (data_loss:0.275 , regularization_loss:0.065), lr: 0.019959093837180697\n",
      "epoch: 4200, acc: 0.887, loss:0.336 (data_loss:0.271 , regularization_loss:0.065), lr: 0.01995809797330505\n",
      "epoch: 4300, acc: 0.891, loss:0.331 (data_loss:0.267 , regularization_loss:0.064), lr: 0.01995710220880218\n",
      "epoch: 4400, acc: 0.893, loss:0.330 (data_loss:0.266 , regularization_loss:0.064), lr: 0.019956106543657228\n",
      "epoch: 4500, acc: 0.891, loss:0.329 (data_loss:0.266 , regularization_loss:0.063), lr: 0.019955110977855316\n",
      "epoch: 4600, acc: 0.892, loss:0.327 (data_loss:0.264 , regularization_loss:0.063), lr: 0.01995411551138158\n",
      "epoch: 4700, acc: 0.893, loss:0.325 (data_loss:0.260 , regularization_loss:0.066), lr: 0.019953120144221154\n",
      "epoch: 4800, acc: 0.895, loss:0.322 (data_loss:0.257 , regularization_loss:0.065), lr: 0.019952124876359174\n",
      "epoch: 4900, acc: 0.896, loss:0.320 (data_loss:0.256 , regularization_loss:0.064), lr: 0.01995112970778079\n",
      "epoch: 5000, acc: 0.896, loss:0.319 (data_loss:0.255 , regularization_loss:0.064), lr: 0.019950134638471142\n",
      "Test Acc: 0.877, loss:0.346\n"
     ]
    }
   ],
   "source": [
    "# Training the neural network with different optimizers\n",
    "\n",
    "X, y = spiral_data(samples=500, classes=3)\n",
    "\n",
    "dense1 = Dense_Layer(2,64, weight_regularizer_l2=5e-4 , bias_regularizer_l2=5e-4)\n",
    "\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "dense2 = Dense_Layer(64,3)\n",
    "activation_2 = Activation_Softmax()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "#optimizer = Optimizer_GD()\n",
    "#optimizer = Optimizer_GD_Decay(decay=1e-3)\n",
    "#optimizer = Optimizer_SGD(decay=1e-3 , momentum=0.9)\n",
    "#optimizer = Optimizer_RMSPROP(learning_rate=0.02 , decay=1e-5 , rho=0.999)\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02 , decay=5e-7)\n",
    "\n",
    "for epoch in range(5001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    regularization_loss = (\n",
    "        loss_activation.loss.regularization_loss(dense1) +\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "    )\n",
    "    loss = data_loss + regularization_loss\n",
    "    predictions = np.argmax(loss_activation.output , axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss:{loss:.3f} (data_loss:{data_loss:.3f} , regularization_loss:{regularization_loss:.3f}), '  + f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    loss_activation.backward(loss_activation.output , y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "dense1.forward(X_test)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output , axis=1)\n",
    "\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'Test Acc: {accuracy:.3f}, ' + f'loss:{loss:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.366, loss:1.099 (data_loss:1.099 , regularization_loss:0.000), lr: 0.05\n",
      "epoch: 100, acc: 0.620, loss:0.881 (data_loss:0.849 , regularization_loss:0.032), lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.637, loss:0.808 (data_loss:0.765 , regularization_loss:0.043), lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.673, loss:0.783 (data_loss:0.737 , regularization_loss:0.045), lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.665, loss:0.774 (data_loss:0.728 , regularization_loss:0.046), lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.673, loss:0.760 (data_loss:0.716 , regularization_loss:0.044), lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.694, loss:0.759 (data_loss:0.715 , regularization_loss:0.044), lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.698, loss:0.709 (data_loss:0.665 , regularization_loss:0.043), lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.689, loss:0.737 (data_loss:0.694 , regularization_loss:0.042), lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.714, loss:0.721 (data_loss:0.679 , regularization_loss:0.041), lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.691, loss:0.727 (data_loss:0.687 , regularization_loss:0.040), lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.693, loss:0.721 (data_loss:0.681 , regularization_loss:0.040), lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.718, loss:0.715 (data_loss:0.675 , regularization_loss:0.040), lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.719, loss:0.698 (data_loss:0.659 , regularization_loss:0.039), lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.709, loss:0.712 (data_loss:0.674 , regularization_loss:0.039), lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.714, loss:0.696 (data_loss:0.658 , regularization_loss:0.039), lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.707, loss:0.690 (data_loss:0.651 , regularization_loss:0.039), lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.695, loss:0.698 (data_loss:0.659 , regularization_loss:0.038), lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.710, loss:0.674 (data_loss:0.636 , regularization_loss:0.038), lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.730, loss:0.669 (data_loss:0.631 , regularization_loss:0.038), lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.719, loss:0.691 (data_loss:0.653 , regularization_loss:0.038), lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.709, loss:0.698 (data_loss:0.660 , regularization_loss:0.038), lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.715, loss:0.696 (data_loss:0.659 , regularization_loss:0.038), lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.733, loss:0.727 (data_loss:0.690 , regularization_loss:0.037), lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.715, loss:0.668 (data_loss:0.631 , regularization_loss:0.037), lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.719, loss:0.729 (data_loss:0.691 , regularization_loss:0.037), lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.723, loss:0.689 (data_loss:0.652 , regularization_loss:0.037), lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.722, loss:0.706 (data_loss:0.669 , regularization_loss:0.037), lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.712, loss:0.715 (data_loss:0.678 , regularization_loss:0.037), lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.704, loss:0.680 (data_loss:0.643 , regularization_loss:0.037), lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.724, loss:0.682 (data_loss:0.645 , regularization_loss:0.038), lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.734, loss:0.641 (data_loss:0.603 , regularization_loss:0.038), lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.706, loss:0.693 (data_loss:0.655 , regularization_loss:0.038), lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.740, loss:0.675 (data_loss:0.637 , regularization_loss:0.038), lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.743, loss:0.651 (data_loss:0.613 , regularization_loss:0.038), lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.728, loss:0.661 (data_loss:0.623 , regularization_loss:0.037), lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.736, loss:0.643 (data_loss:0.604 , regularization_loss:0.038), lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.720, loss:0.672 (data_loss:0.634 , regularization_loss:0.038), lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.708, loss:0.685 (data_loss:0.647 , regularization_loss:0.038), lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.711, loss:0.660 (data_loss:0.623 , regularization_loss:0.037), lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.727, loss:0.665 (data_loss:0.628 , regularization_loss:0.037), lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.737, loss:0.684 (data_loss:0.646 , regularization_loss:0.037), lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.715, loss:0.659 (data_loss:0.621 , regularization_loss:0.038), lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.715, loss:0.654 (data_loss:0.616 , regularization_loss:0.037), lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.716, loss:0.654 (data_loss:0.617 , regularization_loss:0.037), lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.730, loss:0.650 (data_loss:0.614 , regularization_loss:0.037), lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.733, loss:0.654 (data_loss:0.618 , regularization_loss:0.037), lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.716, loss:0.673 (data_loss:0.636 , regularization_loss:0.037), lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.703, loss:0.704 (data_loss:0.668 , regularization_loss:0.036), lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.734, loss:0.647 (data_loss:0.611 , regularization_loss:0.036), lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.727, loss:0.673 (data_loss:0.637 , regularization_loss:0.036), lr: 0.04987533659617785\n",
      "Test Acc: 0.720, loss:0.623\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=500, classes=3)\n",
    "\n",
    "dense1 = Dense_Layer(2,64, weight_regularizer_l2=5e-4 , bias_regularizer_l2=5e-4)\n",
    "dropout1= Layer_Dropout(0.1)\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "dense2 = Dense_Layer(64,3)\n",
    "#dropout2 = Layer_Dropout(0.1)\n",
    "activation_2 = Activation_Softmax()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "#optimizer = Optimizer_GD()\n",
    "#optimizer = Optimizer_GD_Decay(decay=1e-3)\n",
    "#optimizer = Optimizer_SGD(decay=1e-3 , momentum=0.9)\n",
    "#optimizer = Optimizer_RMSPROP(learning_rate=0.02 , decay=1e-5 , rho=0.999)\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05 , decay=5e-7)\n",
    "\n",
    "for epoch in range(5001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dropout1.forward(activation1.output)\n",
    "\n",
    "    dense2.forward(dropout1.output)\n",
    "\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    regularization_loss = (\n",
    "        loss_activation.loss.regularization_loss(dense1) +\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "    )\n",
    "    loss = data_loss + regularization_loss\n",
    "    predictions = np.argmax(loss_activation.output , axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss:{loss:.3f} (data_loss:{data_loss:.3f} , regularization_loss:{regularization_loss:.3f}), '  + f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    loss_activation.backward(loss_activation.output , y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "dense1.forward(X_test)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output , axis=1)\n",
    "\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'Test Acc: {accuracy:.3f}, ' + f'loss:{loss:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
